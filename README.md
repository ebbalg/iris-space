Overview 

This project demonstrates the fine tuning of a LLaMA 3.2 3B model using the QLoRA strategy. In this method, the model's original weights are frozen and small "adapters" matrices are trained. This type of fine tuning drastrically reduces the number of parameters to train, which is significant when starting with LLMs with billions of parameters. QLoRA is an extension of this, where weights are quantized in a compress format, speeding up the entire process.

In this Lab, the LLaMA 3.2 3B model is fine tuned using the FineTome-100k dataset, which is a dataset containing instructions (or questions) and answers. The fine tuned model is serviced in a Gradio app like a chatbot. The chatbot was repurposed to be an AI teacher assistant, helping students answer questions or explain concepts about machine learning. 

To expand the usage of the chatbot we built a quiz that helps students test their understanding of machine learning. The quiz is generated by the fine tune model. It creates 10 multiple choice questions and the answer. The user can select an option and they receive feedback about the answer being correct or not. 

Evaluation

We evaluated the fine-tuned model on the ARC Challenge, a benchmark designed to test reasoning and logic rather than memorization. We used 500 questions from the dataset for practical runtime considerations. The results:

- Fine-tuned model accuracy: 51%
- Baseline LLaMA 3.2 3B accuracy: 67%

The drop in accuracy is expected: fine-tuning encourages the model to follow instructions, which can slightly compromise raw reasoning performance. However, for an AI TA, the ability to reason and provide guided, instructive answers is more important than pure accuracy.

- Fine-tuned model accuracy: X%
- Baseline model accuracy: Y%


Challenges

1. Memory constraints: Running different models with unsloth frequently ran out of RAM when saving to GGUF format. We experimented with various model sizes until one could be successfully saved.

2. Quiz generation and parsing: Crafting a prompt that reliably produced questions, options, and answers in a consistent format was challenging. Correct parsing was essential to ensure the quiz behaved as expected.

3. Model output variability: The modelâ€™s output varied across prompts. While we accounted for most observed formats, some questions may have been correctly answered but incorrectly parsed, potentially underestimating model performance.

